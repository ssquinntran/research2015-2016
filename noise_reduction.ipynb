{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor word in search_words:\\n    predict(word, clamped_to_binary_lang_vecs, \"binary\")\\n    predict(word, clamped_lang_vecs, \"arbitrary cutoff\")\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import random_idx\n",
    "from utils import utils\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import sys\n",
    "import Queue\n",
    "\n",
    "k = 5000\n",
    "N = 10000\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "ordered = 1\n",
    "alph = string.lowercase + ' '\n",
    "\n",
    "def create_lang_vec(cluster_sizes, N=N, k=k):\n",
    "    total_lang = np.zeros((1,N))\n",
    "    # generate english vector\n",
    "    for cz in cluster_sizes:\n",
    "        lang_vector = random_idx.generate_RI_text_fast(N, RI_letters, cz, ordered, \"preprocessed_texts/english/alice_in_wonderland.txt\", alph)\n",
    "        total_lang += lang_vector\n",
    "    return total_lang\n",
    "\n",
    "def plot_letter_frequencies(frequencies, title):\n",
    "    x = np.linspace(0, 26, num=26)\n",
    "    labels = [letter for letter in alph]\n",
    "    plt.figure()\n",
    "    plt.bar(x, frequencies)\n",
    "    plt.xticks(x, labels, rotation='horizontal')\n",
    "    plt.xlabel('Letters')\n",
    "    plt.ylabel('Frequencies')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\"\"\"\n",
    "Clipping basically means limiting or saturating the max/min value of each element in a vector. \n",
    "In other words, we want to reduce the precision of vector. \n",
    "You can use a function like below to limit the max/min value of each element: n is the input value, \n",
    "and bitWidth is the number of bits that can be used to represent each element \n",
    "(bitWidth of 1 makes the vector as a binary vector). \n",
    "\n",
    "\"\"\"\n",
    "def clamp(lang_vec, Min, Max):\n",
    "    for i in range(0, len(lang_vec)):\n",
    "        for j in range(0, len(lang_vec[0])):\n",
    "            if lang_vec[i][j] > Max:\n",
    "                lang_vec[i][j] = Max\n",
    "            elif lang_vec[i][j] < Min:\n",
    "                lang_vec[i][j] = Min\n",
    "    return lang_vec\n",
    "\n",
    "def clamp_to_binary(lang_vec):\n",
    "    #print lang_vec\n",
    "    for i in range(0, len(lang_vec)):\n",
    "        for j in range(0, len(lang_vec[0])):\n",
    "            if lang_vec[i][j] >= 0:\n",
    "                lang_vec[i][j] = 1\n",
    "            else:\n",
    "                lang_vec[i][j] = -1\n",
    "    return lang_vec\n",
    "\n",
    "# predicts the most likely letter for each prefix in the word\n",
    "def predict(word, clamped, type_clamped):\n",
    "    for i in range(len(word)):\n",
    "        prefix = random_idx.id_vector(N, word[:i], alph, RI_letters, ordered)\n",
    "        sprefix = np.roll(prefix, 1)\n",
    "        prefix_ngram = np.multiply(clamped[len(word[:i])+1], sprefix)\n",
    "        frequencies = []\n",
    "        for j in range(26):\n",
    "            frequencies.append(np.dot(prefix_ngram, RI_letters[j]))\n",
    "        plot_letter_frequencies(frequencies, type_clamped + \" for predicting the next letter prefix \" + word[:i] + \" in \" + word)  \n",
    "\n",
    "RI_letters = random_idx.generate_letter_id_vectors(N, k, alph)\n",
    "# lang_vectors in sizes 1-8\n",
    "lang_vectors = []\n",
    "for size in cluster_sizes:\n",
    "    lang_vectors.append(create_lang_vec([size]))\n",
    "lang_vectors.insert(0, np.zeros((1,N)))\n",
    "\n",
    "search_words = [\"foot\", \"runs\", \"consider\", \"vanish\", \"the\", \"she\", \"lady\"]\n",
    "clusters = [np.zeros((1,N)), lang_vectors[1]]\n",
    "for i in range(2, len(lang_vectors)):\n",
    "    clusters.append(np.add(clusters[i-1], lang_vectors[i]))\n",
    "    \n",
    "clamped_to_binary_lang_vecs = [clamp_to_binary(lv) for lv in lang_vectors]\n",
    "# chosen from the number of letters in Alice. \n",
    "# An n-gram can't show up more times than the total number of letters in the text.\n",
    "clamped_lang_vecs = [clamp(lv, -106036, 106036) for lv in lang_vectors]\n",
    "\"\"\"\n",
    "for word in search_words:\n",
    "    predict(word, clamped_to_binary_lang_vecs, \"binary\")\n",
    "    predict(word, clamped_lang_vecs, \"arbitrary cutoff\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creates the load log vector for specified ngrams\n",
    "def load_log_vector(filepath, cluster_sizes, N=N, k=k):\n",
    "    f = open(filepath, \"r\")\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    lang_vecs = []\n",
    "    ngrams_freq = {}\n",
    "    for cz in cluster_sizes:\n",
    "        total_lang = np.zeros((1,N))\n",
    "        for i in range(len(text)):\n",
    "            if i + cz <= len(text):\n",
    "                word = text[i:i+cz]\n",
    "                if word in ngrams_freq.keys():\n",
    "                    ngrams_freq[word] += 1\n",
    "                else:\n",
    "                    ngrams_freq[word] = 1\n",
    "                    \n",
    "                word_vec = random_idx.id_vector(N, word, alph, RI_letters, ordered)\n",
    "                total_lang += np.log2(ngrams_freq[word])*word_vec\n",
    "        lang_vecs.append(total_lang)\n",
    "    return lang_vecs\n",
    "\n",
    "# predicts the most likely letter for each prefix in the word\n",
    "def predict2(word, lang_vecs):\n",
    "    for i in range(len(word)):\n",
    "        prefix = random_idx.id_vector(N, word[:i], alph, RI_letters, ordered)\n",
    "        sprefix = np.roll(prefix, 1)\n",
    "        prefix_ngram = np.multiply(lang_vecs[len(word[:i])+1], sprefix)\n",
    "        frequencies = []\n",
    "        for j in range(26):\n",
    "            frequencies.append(np.dot(prefix_ngram, RI_letters[j]))\n",
    "        plot_letter_frequencies(frequencies, \"log language vectors predict the next letter for prefix \" + word[:i] + \" in \" + word)  \n",
    "\n",
    "# lang_vectors in sizes 1-8\n",
    "log_lang_vecs = load_log_vector(\"preprocessed_texts/english/alice_in_wonderland.txt\", cluster_sizes)\n",
    "predict(\"consider\", log_lang_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
