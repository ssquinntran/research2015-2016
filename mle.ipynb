{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info http://www.nltk.org/nltk_data/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to work with phonemes: http://www.nltk.org/_modules/nltk/corpus/reader/cmudict.html\n",
    "# from nltk.corpus import brown\n",
    "# from nltk.corpus import gutenberg\n",
    "# from nltk.corpus import cmudict\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364871\n",
      "525097\n",
      "3224222\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-203c1420f7a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0mtest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0mn_gram_frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_gram_frequencies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-203c1420f7a4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(freqs, test_indices)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mfwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0mprocessed_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munprocessed_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munprocessed_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_explain_away\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpost_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mdiscovered_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfreqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munprocessed_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-203c1420f7a4>\u001b[0m in \u001b[0;36mdict_explain_away\u001b[0;34m(vocab, text)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstarts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mduplicate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mduplicate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mduplicate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mpreprocessed_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mprocessed_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from utils import random_idx\n",
    "from utils import utils\n",
    "from utils import lang_vectors_utils as lvu\n",
    "\n",
    "k = 5000\n",
    "N = 10000\n",
    "cluster_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "ordered = 1\n",
    "alphabet = string.lowercase + ' '\n",
    "# Normalize\n",
    "RI_letters = random_idx.generate_letter_id_vectors(N, k, alphabet)/float(math.sqrt(N))\n",
    "file_ids = nltk.corpus.gutenberg.fileids()\n",
    "num_iterations = 1\n",
    "\n",
    "def read_text(indices=[]):\n",
    "    words = []\n",
    "    for index in indices:\n",
    "        corpus = [word.encode(\"ascii\").lower() for word in nltk.corpus.gutenberg.words(file_ids[index])]\n",
    "        # preserves indices for testing\n",
    "        start_ignore, end_ignore = -1, -1\n",
    "        for i in range(len(corpus)):\n",
    "            if corpus[i] == \"[\":\n",
    "                start_ignore = i\n",
    "            if corpus[i] == \"]\":\n",
    "                end_ignore = i\n",
    "            if end_ignore != -1 and corpus[i].isalpha():\n",
    "                words.append(corpus[i])\n",
    "    return words\n",
    "    \n",
    "def seed(seed_index):\n",
    "    #n_gram_frequencies = lvu.initialize(cluster_sizes)\n",
    "    n_gram_frequencies = [{} for _ in range(len(cluster_sizes) + 1)]\n",
    "    # save vectors to file\n",
    "    fwrite = open(\"intermediate/n_gram_frequencies\", \"w\")\n",
    "    pickle.dump(n_gram_frequencies, fwrite)\n",
    "    fwrite.close()\n",
    "    words = read_text([seed_index])\n",
    "    \n",
    "    stream = \"\".join(words)\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # nltk freq dist\n",
    "    # http://www.nltk.org/howto/probability.html\n",
    "    fd = nltk.FreqDist(words)\n",
    "    word_freqs = fd.most_common()\n",
    "    #print word_freqs\n",
    "    for wf in word_freqs:\n",
    "        n_gram_frequencies[len(wf[0])][wf[0]] = wf[1]\n",
    "        \n",
    "    lvu.write_data_structures([n_gram_frequencies], [\"intermediate/n_gram_frequencies\"])\n",
    "    return n_gram_frequencies, stream, text\n",
    "    \n",
    "# converts estimated words to an array of text\n",
    "def post_process(tuples, text):\n",
    "    texted = []\n",
    "    for tup in tuples:\n",
    "        word = text[tup[0]:tup[1]]\n",
    "        texted.append(word)\n",
    "    return texted\n",
    "  \n",
    "def unexplained(vocab, text):\n",
    "    matches = [[m.group(0), [m.start(), m.end()-1]] for m in re.finditer(r'\\S+', text)]\n",
    "    unprocessed_words, unprocessed_indices = zip(*matches)\n",
    "    return list(unprocessed_indices), list(unprocessed_words)\n",
    "            \n",
    "# unioned windowing is way too slow.\n",
    "# doesn't do well with new words bc would just recommend the unigrams\n",
    "# the mle will assign the discovered words first and then the unigrams anyways. \n",
    "def dict_explain_away(vocab,text):\n",
    "    print len(text)\n",
    "    # text[:] does not make a new copy\n",
    "    duplicate = \"%s\" % text\n",
    "    #print id(text)\n",
    "    #print id(duplicate)\n",
    "    # make a list of disjoint tuples of (start_index, end_index)\n",
    "    preprocessed_indices = []\n",
    "\n",
    "    for i in range(len(vocab)-1, 1,-1):\n",
    "        for key in vocab[i].keys():\n",
    "            starts = [match.start() for match in re.finditer(re.escape(key), duplicate)]\n",
    "            for start in starts:\n",
    "                mask = \" \"*len(key)\n",
    "                duplicate = duplicate[:start] + mask + duplicate[start+len(key):]\n",
    "                preprocessed_indices.append([start, start + len(key)])\n",
    "    processed_indices = [[pair[0],pair[1]] for pair in preprocessed_indices if pair[0] < pair[1]]\n",
    "    unprocessed_indices, unprocessed_words = unexplained(vocab, duplicate)\n",
    "    return sorted(processed_indices), unprocessed_indices, unprocessed_words\n",
    "\n",
    "\"\"\"\n",
    "how do you choose between sticking a word in a category and with adding a new word. how to choose the threshold.\n",
    "threshold is if most matching word with ngram is < .5 sd away from ngram dot ngram. \n",
    "this means we need to have saved a distribution of word dot word for every word in the model\n",
    "the model is NOT a similarity matrix. only O(N) space and O(N) time with constant cost per operation\n",
    "these are all non matched words.\n",
    "\"\"\"\n",
    "def mle(freqs, words):\n",
    "    predicted = []\n",
    "    # need a dot product matrix\n",
    "    for word in words:\n",
    "        # need to normalize\n",
    "        word_vector = random_idx.id_vector(N, word, alphabet, RI_letters, ordered)/float(len(word))\n",
    "        self_dot = np.dot(word_vector, word_vector.T)\n",
    "        cluster_size = len(word) # or + 1?\n",
    "        max_sim = float(\"-inf\")\n",
    "        max_key = \"\"\n",
    "        \n",
    "        # for optimization, don't go all the way down. go down like 4 letters max\n",
    "        for i in range(cluster_size, max(cluster_size-5,-1),-1):\n",
    "            for key in freqs[i].keys():\n",
    "                key_vector = random_idx.id_vector(N, key, alphabet, RI_letters, ordered)#/float(len(key))\n",
    "                similarity = np.dot(word_vector, key_vector.T)\n",
    "                if similarity > max_sim:\n",
    "                    max_sim = similarity\n",
    "                    max_key = key\n",
    "        # random vectors of the same length self dot products. a sample each time\n",
    "        model_self_similarities = np.zeros(10)\n",
    "        keys = freqs[cluster_size].keys()[:10]\n",
    "        for i in range(10):\n",
    "            wv = random_idx.id_vector(N, keys[i], alphabet, RI_letters, ordered)/float(len(keys[i]))\n",
    "            model_self_similarities[i] = wv.dot(wv.T)\n",
    "            \n",
    "        if abs(self_dot - max_sim) < np.std(model_self_similarities, axis=0):\n",
    "            #print \"categorized\"\n",
    "            freqs[len(max_key)][max_key] += 1\n",
    "        else:\n",
    "            #print \"new word\"\n",
    "            max_key = word\n",
    "            freqs[len(word)][word] = 1\n",
    "        \n",
    "        \n",
    "        predicted.append(max_key)\n",
    "    \n",
    "    \n",
    "    return predicted\n",
    "\n",
    "def predict_text(textname, processed_indices, processed, unprocessed_indices, discovered_words):\n",
    "    # index in the actual text to index in processed \"p\" or unprocessed array \"u\"\n",
    "    indices = {}\n",
    "    texti = []\n",
    "    text = \"\"\n",
    "    for i in range(len(processed_indices)):\n",
    "        indices[processed_indices[i][0]] = [i, \"p\"]\n",
    "    for i in range(len(unprocessed_indices)):\n",
    "        indices[unprocessed_indices[i][0]] = [i, \"u\"]\n",
    "    keys = sorted(indices.keys())\n",
    "    for k in keys:\n",
    "        pair = indices[k]\n",
    "        if pair[1] == \"p\":\n",
    "            texti.append(processed[pair[0]])\n",
    "        else:\n",
    "            texti.append(discovered_words[pair[0]])\n",
    "        text += texti[len(texti)-1] + \" \"\n",
    "    fwrite = open(\"output/\" + textname, \"w\")\n",
    "    fwrite.write(text)\n",
    "    fwrite.close()\n",
    "    return texti\n",
    "    \n",
    "    \n",
    "def predict(freqs, test_indices):\n",
    "    for ti in test_indices:   \n",
    "        words = read_text([ti])\n",
    "        stream = \"\".join(words)\n",
    "        text = \" \".join(words)\n",
    "        fwrite = open(\"input/\" + file_ids[ti], \"w\")\n",
    "        fwrite.write(text)\n",
    "        fwrite.close()\n",
    "        processed_indices, unprocessed_indices, unprocessed_words = dict_explain_away(freqs,stream)\n",
    "        processed = post_process(processed_indices, stream)\n",
    "        discovered_words = mle(freqs, unprocessed_words)\n",
    "        predict_text(file_ids[ti], processed_indices, processed, unprocessed_indices, discovered_words)\n",
    "    \n",
    "    # save data to file\n",
    "    lvu.write_data_structures([n_gram_frequencies], [\"intermediate/n_gram_frequencies\"])\n",
    "    \n",
    "seed_index = 0\n",
    "test_indices = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "n_gram_frequencies, stream, text = seed(seed_index)\n",
    "predict(n_gram_frequencies, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_ids = nltk.corpus.gutenberg.fileids()\n",
    "inds = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for ti in test_indices:   \n",
    "    words = read_text([ti])\n",
    "    stream = \"\".join(words)\n",
    "    text = \" \".join(words)\n",
    "    fwrite = open(\"input/\" + file_ids[ti], \"w\")\n",
    "    fwrite.write(text)\n",
    "    fwrite.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
